# 强化学习1

在本系列内容中，我们会涉及到许多概率论的知识，首先需要统一符号的含义：

$$
大写的X代表随机变量,小写的x代表随机变量的观测值
$$

1. 概率密度函数：对于某一随机变量，其在某个确定的取值点附近的可能性。概率密度函数的性质：

$$
对于随机变量X，其落在域 \mathcal{X} 中
$$

$$
\int_{\mathcal{X}} p(x) dx = 1其中p(x)就是概率密度函数
$$

$$
对于离散的概率密度函数:\sum_{X \in \mathcal{X}} p(x) = 1
$$

2. 期望：

随机变量X，其期望可以如下表示：
$$
X \in \mathcal{X}
$$

$$
对于连续随机变量:\mathbb{E}[f(X)] = \int_{\mathcal{X}} p(x) f(x) dx
$$

$$
对于离散随机变量:\mathbb{E}[f(X)] = \sum_{x \in \mathcal{X}}p(x)f(x)
$$

3. 随机抽样

随机变量X取到某个观测值x的过程就叫随机抽样，例如：箱子里有2个红色的球、5个绿色的球、3个蓝色的球，这些球除了颜色外没有任何区别，随机变量X的观测值可能是（红色球、绿色球、蓝色球），伸手进去摸这个球的过程就叫做随机抽样。或例如：箱子里有一些球，取到红色球的概率为0.2、绿色球的概率为0.5、蓝色球的概率为0.3。这两个例子本质上是一样的，第一个例子中，只要进行足够的随机抽样，统计概率就与第二个例子中一样了。

## 强化学习的一些专业术语

1. Agent——智能体：在强化学习中，通过与环境的交互来最大化累积奖励的个体。
2. state——状态：智能体与环境当前的属性以及动作等
3. Action——动作：智能体来积累奖励可执行的操作
4. policy——策略（Π）：智能体采用何种动作来积累奖励

​	在数学上，policy的定义如下（Π是一个概率密度函数，指采取某种动作的概率）：

$$
\pi (a|s) = P(A = a | S = s)
$$

​	即在状态为状态观测量s的条件下，采取a动作的概率。有一系列的动作可以执行，因此在同一个状态下对应的Π可以有多个，不同的动作对应不同的概率。再对这些动作进行随机抽样，以做出行动，这就是策略。（为什么要进行随机抽样而不是将某个状态观测量下的动作固定，这是为了更好地与其他智能体相搏弈，如果动作固定，则很容易被预判到）

5. reward——奖励（R）：智能体每做出一个动作都会给智能体一个奖励。奖励是用户自己定义的，奖励定义的好坏会直接影响强化学习的性能。
6. state transition——状态转移：智能体每做出一个动作就会进行一个状态转移，从旧的状态转移到新的状态。状态的转移是随机的（随机性从环境以及动作的随机性中来）。可以用下面的概率来表示：

$$
p(s^{`} | s,a)=P(S^{`}=s^{`} | S = s , A = a)
$$

​	上面这个概率表示：在当前状态S为观测量s且采取了动作a的条件下，接下来的状态S变为观测量s`的概率。

智能体与环境的交互可以用下图示意

![1](/image/1.png)

**这里最重要的两个概率**：

$$
动作概率密度:\pi(a|s);状态转移概率:p(s^{`}|s,a)
$$

强化学习中“学习”的内容：学习策略policy函数Π，以此来将当前状态下最优的动作概率提升到最高。

7. trajectory——轨迹：在状态转移过程中所留下的状态、动作、奖励的轨迹：

$$
(s_1,a_1,r_1) \rightarrow (s_2,a_2,r_2) \rightarrow ... \rightarrow (s_T,a_T,r_T)
$$

## 几个重要的概念

### 回报——return

回报也可以称作未来的累计奖励，将t时刻的回报记作U_t

$$
U_t = R_t + R_{t+1} + R_{t+2} + R_{t+3}+...
$$

有的时候，每个时刻回报的重要性不完全一样，例如：当前就给你100块钱和一年后给你100块钱，两个回报的重要性是不一样的。未来时刻的奖励相比于当下的奖励而言，重要性就没那么高了，因此可以给当前奖励和未来奖励一个权重，未来奖励的权重要比当前奖励的权重要小，这个权重叫做：折扣率，记作γ；增加了折扣率的回报也叫做折扣回报。

$$
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3}+...,\gamma \in [0,1]
$$

折扣率是一个超参数，需要用户自己设置，折扣率的不同对于学习的效果也会有不同程度的影响。

【注】折扣是一个随机变量，若学习已经结束了，则所有的奖励和回报都是确定的数值，这个时候他们就是观测量，用小写字母表示。

在任意时刻t，回报U_t是随机的，其随机性有两个来源：动作A与下一个状态S`。

$$
\mathbb{P}(A=a|S=s) = \pi (a|s)
$$

$$
\mathbb{P} (S^{`}=s^{`} | S=s , A=a) = p(s^{`}|s,a)
$$

对于任意时刻的i，其奖励R_i取决于i时刻的状态S_i和动作A_i。因此回报也与未来时刻所有的状态以及动作有关。

假设已经给定当前时刻的状态S_t，回报取决于：

$$
A_t,A_{t+1},A_{t+2},...和S_{t+1},S_{t+2},...(S_t 给定)
$$

### 动作价值函数——Action-Value Function——Q(s,a)

折扣回报：

$$
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3}+...,\gamma \in [0,1]
$$

Agent的目标就是让U_t越大越好。由于U_t是个随机变量，在t时刻我们并不知道U_t是什么，只知道它依赖于未来的动作和状态，那要怎么评估当前的形势呢？可以对U_t求期望

$$
Q_{\pi}(s_t, a_t) = \mathbb{E}(U_t|S_t=s_t, A_t=a_t)
$$

把U_t当作未来所有的状态s和所有动作a的一个函数，未来所有的状态和动作都有随机性：

$$
动作随机(策略函数),动作的概率密度函数是\text{policy}函数\pi:\mathbb{P}(A=a|S=s) = \pi (a|s)
$$

$$
状态随机(状态转移),状态转移的概率密度函数是状态转移函数p:\mathbb{P}(S^{`}=s^{`} | S=s , A=a) = p(s^{`}|s,a)
$$

期望就是对未来的状态和动作求的，在求期望的过程中，使用积分消除未来的状态和动作，只剩下当前的状态和动作S_t和A_t。

$$
被消除的变量为未来的动作A_{t+1},A_{t+2},...和未来的状态S_{t+1},S_{t+2},...
$$

求期望得到的Q_Π叫做：Action-Value Function——动作价值函数，Q__Π只与当前状态st和当前动作at有关，st和at是观测量。同时价值函数QΠ还与policy函数Π有关，因为在积分的时候会用到policy函数。

动作价值函数QΠ可以告诉我们，在当前时刻状态为st的条件下，采取at这个动作的效果是好还是坏，并给当前状态下所有的动作a打分，这样就可以知道哪个动作好、哪个动作不好。

因此有：最优动作价值函数——Optimal action-value function——Q^*(s_t,a_t)

$$
Q^{*}(s_t,a_t)=\max_{\pi}Q_{\pi}(s_t,a_t)
$$

### 状态价值函数——State-Value Function——V(s)

在这里动作A是随机变量，可以对动作价值函数关于A求期望，将动作消除：

$$
V_{\pi}(s_t) = E_{A} (Q_{\pi}(s_t,A))
$$

此时求得的V_Π(st)只与st和Π有关。

V_Π(st)的意义：告诉我们当前的局势是好还是不好。例如：根据policy函数来下围棋，状态价值函数就可以告诉我们当前局势好还是不好，胜算有多大。

由于动作是离散的，因此可以如下计算：

$$
V_{\pi}(s_t) = E_A (Q_{\pi}(s_t,A))=\sum_a \pi(a|s_t)Q_{\pi}(s_t,a)
$$

也有的情况下动作需要是连续的，如自动驾驶汽车的方向盘、油门等需要连续变化，就需要如下计算：

$$
V_{\pi}(s_t) = E_A (Q_{\pi}(s_t,A))=\int \pi(a|s_t)Q_{\pi}(s_t,a)da
$$

并且状态价值函数还可以评价policy函数Π的好与坏：

$$
E_S (V_{\pi}(S)),当\pi越好,E_S (V_{\pi}(S))就越大,反之就越小
$$

【注】AI控制Agent的方法有：
针对policy函数进行学习——policy based learning——策略学习：使用Π函数控制agent做动作，即基于观测状态s_t：

$$
\textcolor{#FF0000}{a_t} \sim \pi (\textcolor{#FF0000}{\cdot}|s_t)
$$

针对最优动作价值函数Q*来进行学习——value based learning——价值学习：使用最优动作价值函数来控制agent做动作，即基于观测状态s_t：

$$
\textcolor{#FF0000}{a_t} = \text{argmax}_{\textcolor{#FF0000}{a}} Q^*(s_t,\textcolor{#FF0000}{a})
$$

所以强化学习的任务就是学习到这两个函数之一，让agent能够按照某种规律达到我们想要的效果即可。

针对强化学习有一个标准库：OpenAI Gym，在设计出一个强化学习的算法之后，需要有一个标准来检验这个算法是否比其他强化学习的算法要好，因此所有强化学习的算法都会使用标准数据库Gym来检验算法。但现在OpenAI Gym这个库已经更新为Gymnasium，官网为：https://gymnasium.farama.org/

安装Gymnasium的方法：

```bash
pip install gymnasium
pip install "gymnasium[classic-control]"
```

然后使用如下代码检查是否安装成功：

```python
import gymnasium as gym

env = gym.make('CartPole-v1', render_mode='human')
state, info = env.reset()

for t in range(100):
    env.render()
    print(state)

    action = env.action_space.sample()
    state, reward, terminated, truncated, info = env.step(action)

    if terminated
        print('FINISHED')
        break

env.close()
```

如果成功安装则会短暂弹出一个窗口，如下所示

![](/image/2.png)





